import torch.nn as nn
import torch
import torch.nn.functional as F
from util.pos_embed import get_2d_sincos_pos_embed_from_grid


class GeoLink_Fusion_Embedding(nn.Module):

    '''
    You can use this class to obtain the fusion embeddings.
    The default output embeedings are from the 7/11/15th layers of ViT-L and the hybrid RS-OSM embeddings.
    The hybrid RS-OSM embeddings are generated by the integration of features from last(23th) layer of ViT-L
    and OSM embeddings (detail structure are shown in the right of Fig 8b in the paper).
    Such embeddings can be applied to downstream tasks through a task-specific decoder, like UpperNet for semantic segmentation.
    '''

    def __init__(
            self,
            fusion_model,
            output_layers = [7, 11, 15, 23],
            pyramid_output=False,
    ):
        super().__init__()
        self.img_encoder = fusion_model.img_encoder
        self.osm_encoder = fusion_model.osm_encoder
        self.output_layers = output_layers
        self.pyramid_output = pyramid_output

        self.fusion_embed_dim = fusion_model.fusion_embed_dim
        self.img_embed_dim = self.img_encoder.embed_dim
        self.patch_embed = self.img_encoder.patch_embed

        self.output_dim = [self.img_embed_dim]*3 + [self.fusion_embed_dim]
        self.img_fusion_mlp = fusion_model.img_fusion_mlp
        self.img_norm = fusion_model.img_norm

        self.osm_node_fusion_mlp=fusion_model.osm_node_fusion_mlp
        self.osm_norm = fusion_model.osm_norm
        self.fusion_blocks = fusion_model.osm_cross_modal_encoder
        self.patch_size = self.img_encoder.patch_embed.patch_size[0]
        self.img_pos_embed = fusion_model.img_pos_embed
        self.cls_token = fusion_model.cls_token
        self.fusion_img_pos_embed = fusion_model.fusion_img_pos_embed
        self.num_patches=self.img_encoder.patch_embed.num_patches


    def get_node_pos(self, node_pos, t, embed_dim, device):
        if t == 'polygon':
            node_pos = node_pos * (self.patch_embed.num_patches ** .5)
            node_pos = node_pos.reshape(-1, 4, 2)
            node_pos = node_pos.transpose(2, 0, 1).reshape(2, -1)
            node_pos_embedding = get_2d_sincos_pos_embed_from_grid(embed_dim, grid=node_pos)
            node_pos_embedding = node_pos_embedding.reshape(-1, 4, embed_dim)
            node_pos_embedding = node_pos_embedding.mean(axis=1)
            node_pos_embedding = torch.from_numpy(node_pos_embedding).to(device).float()
            return node_pos_embedding
        elif t == 'line':
            node_pos = node_pos * (self.patch_embed.num_patches ** .5)
            node_pos = node_pos.reshape(-1, 3, 2)
            node_pos = node_pos.transpose(2, 0, 1).reshape(2, -1)
            node_pos_embedding = get_2d_sincos_pos_embed_from_grid(embed_dim, grid=node_pos)
            node_pos_embedding = node_pos_embedding.reshape(-1, 3, embed_dim)
            node_pos_embedding = node_pos_embedding.mean(axis=1)
            node_pos_embedding = torch.from_numpy(node_pos_embedding).to(device).float()
            return node_pos_embedding
        elif t == 'point':
            node_pos = node_pos * (self.patch_embed.num_patches ** .5)  # relative pos to absolute pos+
            node_pos = node_pos.transpose(1, 0)
            node_pos_embedding = get_2d_sincos_pos_embed_from_grid(embed_dim, grid=node_pos)
            node_pos_embedding = torch.from_numpy(node_pos_embedding).to(device).float()
        return node_pos_embedding

    def pad_node(self, node_features, node_pos_embed, batch, max_sample_nodes = 50):
        num_graphs = batch.max().item() + 1
        node_counts = batch.bincount()  # Number of nodes per graph
        max_nodes = min(node_counts.max(dim=0)[0], max_sample_nodes)
        padding_mask = torch.arange(max_nodes, device=node_counts.device).unsqueeze(0) < node_counts.unsqueeze(1)  # (num_graphs, max_nodes) from True to False

        padded_node_features = torch.zeros((num_graphs, max_nodes, node_features.size(1)), device=node_features.device)
        padded_node_pos_embed = torch.zeros((num_graphs, max_nodes, node_pos_embed.size(1)), device=node_features.device)
        # Fill in padded_node_features using the batch tensor
        for i in range(num_graphs):
            # Get the indices of the nodes that belong to the i-th graph
            graph_node_indices = (batch == i).nonzero(as_tuple=True)[0]
            # # if more than max_nodes, random select max_nodes
            if graph_node_indices.shape[0] > max_nodes:
                graph_node_indices = graph_node_indices[torch.randperm(graph_node_indices.shape[0])[:max_nodes]]

            # Copy the node features into the padded tensor
            padded_node_features[i, :len(graph_node_indices)] = node_features[graph_node_indices]
            padded_node_pos_embed[i, :len(graph_node_indices)] = node_pos_embed[graph_node_indices]

        return padded_node_features, padded_node_pos_embed, padding_mask

    def forward_img_encoder(self, x):
        # embed patches
        x = self.img_encoder.patch_embed(x)

        # add pos embed w/o cls token
        x = x + self.img_pos_embed[:, 1:, :]

        # append cls token
        cls_token = self.cls_token + self.img_pos_embed[:, :1, :]
        cls_tokens = cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        # apply Transformer blocks
        output = []
        for i, blk in enumerate(self.img_encoder.blocks):
            x = blk(x)
            if i in self.output_layers:
                output.append(x[:, 1:])
        return output

    def forward_osm_encoder(self, osms):

        out = self.osm_encoder(osms)
        out = {key: self.osm_norm[key](x) for key, x in out.items()}

        return out

    def forward_cross_encoder(self, osms, osm_node_embeddings, img_embedding, node_pos):
        device = self.img_pos_embed.device

        node_pos_embedding, osm_fusion_embedding, batch_index = [], [], []
        for t in osms.node_types:
            node_pos_embedding_t = self.get_node_pos(node_pos[t], t, self.fusion_embed_dim, device)

            # get osm mask embeddings
            osm_embedding_t = osm_node_embeddings[t]
            osm_fusion_t = self.osm_node_fusion_mlp[t](osm_embedding_t)

            batch_index_t = osms[t].batch

            node_pos_embedding.append(node_pos_embedding_t)
            osm_fusion_embedding.append(osm_fusion_t)
            batch_index.append(batch_index_t)

        node_pos_embedding = torch.concat(node_pos_embedding, dim=0)
        osm_fusion_embedding = torch.concat(osm_fusion_embedding, dim=0)
        batch_index = torch.concat(batch_index, dim=0)

        padded_osm_mask_embedding, padded_node_pos_embedding, padding_mask = \
            self.pad_node(osm_fusion_embedding, node_pos_embedding, batch_index)  # B*mask_node_num*dim

        # repeat img mask embeddings
        img_embedding_fusion = self.img_fusion_mlp(self.img_norm(img_embedding))
        # apply Cross-modal Transformer blocks
        for blk in self.fusion_blocks:
            img_embedding, osm_mask_embedding = blk(img_embedding_fusion, padded_osm_mask_embedding,
                                                    self.fusion_img_pos_embed[:, 1:],
                                                    padded_node_pos_embedding, ~padding_mask)

        return img_embedding

    def forward(self, im, osms):
        device = self.img_pos_embed.device
        node_pos = {}
        for t in osms.node_types:
            if t == 'polygon':
                node_pos[t] = osms[t].x[:, 512:520].clone().detach().cpu().numpy()
                osms[t].x = osms[t].x[:, :512]
            elif t == 'line':
                node_pos[t] = osms[t].x[:, 512:518].clone().detach().cpu().numpy()
                osms[t].x = osms[t].x[:, :512]
            elif t == 'point':
                node_pos[t] = osms[t].x[:, 512:514].clone().detach().cpu().numpy()
                osms[t].x = osms[t].x[:, :512]

        img_embeddings = self.forward_img_encoder(im)
        osm_embeddings = self.forward_osm_encoder(osms)

        img_embeddings[-1] = self.forward_cross_encoder(osms, osm_embeddings, img_embeddings[-1], node_pos)

        for i, img_embedding in enumerate(img_embeddings):
            # print(img_embedding.shape)
            img_embeddings[i] = (
                img_embedding.transpose(1, 2)
                    .view(
                    img_embedding.shape[0],
                    -1,
                    im.shape[2] // self.patch_size,
                    im.shape[3] // self.patch_size,
                )
                    .contiguous()
            )

        return img_embeddings # list size = [4, batch_size, embed_dim, 14, 14]