{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bdc2f6-788a-4d86-bf55-44db4e6433f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import collections\n",
    "from model import *\n",
    "from dataset import *\n",
    "import os\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea24e2f-8de2-49f5-9120-bb77b3e93f71",
   "metadata": {},
   "source": [
    "# for unimodal ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4d515-b48b-40a4-a67b-bf29d5b7def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "model_path = 'geolink_vit_large_patch16_224.pth' # path to ViT checkpoint dir\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "config = checkpoint['model_config']\n",
    "\n",
    "model = timm.create_model(\n",
    "    config['architecture'], \n",
    "    pretrained=False, \n",
    "    num_classes=config['num_classes'], \n",
    "    global_pool=config['global_pool']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# using sinusoidal position embedding with no back-propogation\n",
    "model.pos_embed.requires_grad = config['pos_embed_requires_grad'] \n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766f92-5091-4a46-baa3-5e5d7f901a9a",
   "metadata": {},
   "source": [
    "# for multimodal GeoLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec119d-2779-4969-9358-ebf9d683dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from model import *\n",
    "from dataset import *\n",
    "\n",
    "'''\n",
    "    You can use the following code to obtain the fusion embeddings.\n",
    "    The default output embeedings are from the 7/11/15th layers of ViT-L and the hybrid RS-OSM embeddings.\n",
    "    The hybrid RS-OSM embeddings are generated by the integration of features from last(23th) layer of ViT-L\n",
    "    and OSM embeddings (detail structure are shown in the right of Fig 8b in the paper).\n",
    "    Such embeddings can be applied to downstream tasks through a task-specific decoder, like UperNet for semantic segmentation.\n",
    "'''\n",
    "\n",
    "ckpt_fp = r'geolink_mutimodal_vit_large_patch16_224.pth' # path to multimodal geolink checkpoint dir\n",
    "checkpoint = torch.load(ckpt_fp, map_location='cpu')\n",
    "config = checkpoint['model_config']\n",
    "img_encoder = timm.create_model(\n",
    "    config['architecture'], \n",
    "    pretrained=False, \n",
    "    num_classes=config['num_classes'], \n",
    "    global_pool=config['global_pool']\n",
    ")\n",
    "\n",
    "osm_encoder = OSMHeteroGAT()\n",
    "geolink = GeoLink(img_encoder, osm_encoder)\n",
    "msg = geolink.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(msg)\n",
    "\n",
    "multi_encoder = GeoLink_Fusion_Embedding(geolink, output_layers=[7, 11, 15, 23]) # 23 means the fusion embedding with shape of [256,14,14]\n",
    "\n",
    "data_root = r'./example_data/ufz_example'\n",
    "samples = os.listdir(os.path.join(data_root, 'graph'))\n",
    "\n",
    "trainset = DownstreamDataset(data_root, file_names=samples)\n",
    "trainloader = torch_geometric.data.DataLoader(trainset, batch_size=2,\n",
    "                             pin_memory=True, num_workers=1, drop_last=True, shuffle=True)\n",
    "train_iter = iter(trainloader)\n",
    "# 获取一个batch\n",
    "img, graph = next(train_iter)\n",
    "result = multi_encoder(img, graph) #list size [4, batch_size, embed_dim, 14, 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08147628-c20f-4899-a27a-f3de34c21c47",
   "metadata": {},
   "source": [
    "# for multimodal segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d43b8-d06b-4d29-aeda-c5c8f5baa3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from model import *\n",
    "from dataset import *\n",
    "\n",
    "ckpt_fp = r'geolink_mutimodal_vit_large_patch16_224.pth' # path to multimodal geolink checkpoint dir\n",
    "checkpoint = torch.load(ckpt_fp, map_location='cpu')\n",
    "config = checkpoint['model_config']\n",
    "img_encoder = timm.create_model(\n",
    "    config['architecture'], \n",
    "    pretrained=False, \n",
    "    num_classes=config['num_classes'], \n",
    "    global_pool=config['global_pool']\n",
    ")\n",
    "\n",
    "osm_encoder = OSMHeteroGAT()\n",
    "geolink = GeoLink(img_encoder, osm_encoder)\n",
    "msg = geolink.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(msg)\n",
    "\n",
    "multi_encoder = GeoLink_Fusion_Embedding(geolink, output_layers=[7, 11, 15, 23])\n",
    "model = SegUPerNet(encoder=multi_encoder, num_classes=9, channels=512)\n",
    "\n",
    "data_root = r'./example_data/ufz_example'\n",
    "samples = os.listdir(os.path.join(data_root, 'graph'))\n",
    "\n",
    "trainset = DownstreamDataset_UFZ(data_root, file_names=samples)\n",
    "trainloader = torch_geometric.data.DataLoader(trainset, batch_size=2,\n",
    "                             pin_memory=True, num_workers=1, drop_last=True, shuffle=True)\n",
    "train_iter = iter(trainloader)\n",
    "# 获取一个batch\n",
    "img, graph, label = next(train_iter)\n",
    "result = model(img, graph) #list [batch_size, cls_num, 224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f69ee-4023-4cdd-bd23-3fa7a35375a5",
   "metadata": {},
   "source": [
    "# prepare OSM graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d501f-7230-4361-aa87-ff64be54502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = r'./example_data/osm2graph_example/' # path to the osm example dir\n",
    "name = 'w221548378_US_21'\n",
    "polygon_file = gpd.read_file(os.path.join(example_path, name, name + '_polygon.geojson'))\n",
    "polygon_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979567ea-b7f5-4609-8fa0-dc3aee09b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pickle\n",
    "import os\n",
    "from prepare_data import *\n",
    "\n",
    "example_path = r'./example_data/osm2graph_example/' # path to the osm example dir\n",
    "tagw_path = r'all_tags30_frequency1.json'\n",
    "\n",
    "osm_process = OSM2Graph(tagw_path, 'cuda:1')\n",
    "name = 'a472140250_FI_21' # example name\n",
    "\n",
    "# obatin the geographic boundingbox of the give RS image\n",
    "# here, the boundingbox is saved in the meta file\n",
    "# you can also calculate it from the original RS data like TIFF, tools are prepared in the prepare_data/utils.py\n",
    "# note that the geographic coodinate system should be the same between RS and OSM data\n",
    "\n",
    "with open(os.path.join(example_path, name, name+'.pickle'), 'rb') as file:\n",
    "    meta_data = pickle.load(file)\n",
    "bbox = meta_data['bbox']\n",
    "north, south, east, west = bbox[3], bbox[1], bbox[2], bbox[0]\n",
    "width = east - west\n",
    "height = north - south\n",
    "\n",
    "# OSM2Graph can handle situations where one or two vector types are missing\n",
    "# If none of the three vector types are available, then just use RS image.\n",
    "try:\n",
    "    polygon_file = gpd.read_file(os.path.join(example_path, name, name + '_polygon.geojson'))\n",
    "except:\n",
    "    polygon_file = None\n",
    "try:\n",
    "    line_file = gpd.read_file(os.path.join(example_path, name, name + '_line.geojson'))\n",
    "except:\n",
    "    line_file = None\n",
    "try:\n",
    "    point_file = gpd.read_file(os.path.join(example_path, name, name + '_point.geojson'))\n",
    "except:\n",
    "    point_file = None\n",
    "\n",
    "data = osm_process.process(polygon_file, line_file, point_file, north, south, east, west)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205e8be-e9a7-4340-8f41-64bcf7de1662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rose",
   "language": "python",
   "name": "rose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
